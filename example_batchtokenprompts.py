#Generated by Gemini
from vllm import LLM, SamplingParams
from transformers import AutoTokenizer

# 1. Load the model and its tokenizer
# It's good practice to load the tokenizer separately if you plan to use it explicitly
# in your application, even if vLLM handles it internally.
model_name = "/mnt/my_mount/mlperf/Qwen" # Or any other model supported by vLLM

# Initialize vLLM LLM instance
# For simplicity, we let vLLM handle the model loading from Hugging Face Hub
# (ensure you have it downloaded locally or an internet connection for the first run)
llm = LLM(model=model_name)

# Load the tokenizer separately for explicit tokenization
# Ensure trust_remote_code=True if the tokenizer requires it
tokenizer = AutoTokenizer.from_pretrained(model_name)

# 2. Prepare your text prompts
text_prompts = [
    "Hello, my name is",
    "The capital of France is",
    "What is the square root of 64?",
]

# 3. Pre-tokenize the prompts to get token IDs
# You typically add a BOS (Beginning of Sentence) token if the model expects it,
# and ensure you handle padding/truncation as needed for your specific use case.
# For simple generation, just encoding is often enough.
prompt_token_ids_list = []
for text_prompt in text_prompts:
    # `add_special_tokens=True` includes BOS (and EOS if applicable)
    token_ids = tokenizer.encode(text_prompt, add_special_tokens=True)
    prompt_token_ids_list.append(token_ids)

print("Original Text Prompts:")
for text in text_prompts:
    print(f"- {text}")

print("\nPre-tokenized Prompt Token IDs (first 3):")
for i, token_ids in enumerate(prompt_token_ids_list[:3]):
    print(f"- Prompt {i}: {token_ids}")

# 4. Define sampling parameters
sampling_params = SamplingParams(
    temperature=0.8,
    top_p=0.95,
    max_tokens=50, # Max tokens to generate per prompt
)

# 5. Pass the pre-tokenized prompt tokens to vLLM's generate method
# Note: You use 'prompt_token_ids' instead of 'prompts'
print("\nGenerating text with vLLM using prompt_token_ids...")
outputs = llm.generate(prompt_token_ids=prompt_token_ids_list, sampling_params=sampling_params)

# 6. Print the generated outputs
print("\n--- Generated Outputs ---")
for i, output in enumerate(outputs):
    original_prompt_text = text_prompts[i] # Retrieve original text if needed
    generated_text = output.outputs[0].text
    print(f"Prompt: {original_prompt_text!r}")
    print(f"Generated text: {generated_text!r}\n")
